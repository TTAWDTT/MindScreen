"""
MindScreen - å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒè„šæœ¬ (ä¼˜åŒ–ç‰ˆ)
å¯¹æ¯”å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œé€‰æ‹©æœ€ä½³æ¨¡å‹å¹¶è®°å½•è®­ç»ƒç»“æœ

æ”¯æŒçš„æ¨¡å‹ï¼š
- Logistic Regression
- Random Forest
- XGBoost
- LightGBM (å¯é€‰)
- SVM (å¯é€‰)

ä¼˜åŒ–ç‰¹æ€§ï¼š
- å¼‚æ­¥è¿›åº¦æ˜¾ç¤ºï¼Œè®­ç»ƒè¿‡ç¨‹æ— å¡é¡¿
- æ¸…æ™°çš„ç»“æ„åŒ–æ—¥å¿—è¾“å‡º
- å®æ—¶è¿›åº¦æ¡ä¸è€—æ—¶ç»Ÿè®¡

è¾“å‡ºï¼š
- æœ€ä½³æ¨¡å‹æ–‡ä»¶ (.pkl)
- è¯¦ç»†è®­ç»ƒæŠ¥å‘Š (model_comparison_report.json)
"""

import json
import os
import sys
import time
import warnings
import threading
from typing import Dict, Any, Tuple, List, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (
    balanced_accuracy_score, f1_score, classification_report,
    roc_auc_score, precision_score, recall_score
)
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
import joblib

# ========== å…¨å±€é…ç½® ==========
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# å¯é€‰ä¾èµ–æ£€æµ‹
try:
    from xgboost import XGBClassifier
    HAS_XGB = True
except ImportError:
    HAS_XGB = False

try:
    from lightgbm import LGBMClassifier
    HAS_LGBM = True
except ImportError:
    HAS_LGBM = False

# ========== è·¯å¾„é…ç½® ==========
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.abspath(os.path.join(BASE_DIR, "..", "smmh.csv"))
MODEL_DIR = os.path.abspath(os.path.join(BASE_DIR, "..", "models"))


# ========== ç¾åŒ–è¾“å‡ºå·¥å…·ç±» ==========
class ProgressPrinter:
    """è®­ç»ƒè¿›åº¦æ‰“å°å™¨ï¼Œæä¾›æµç•…çš„ç»ˆç«¯è¾“å‡ºä½“éªŒ"""
    
    # çŠ¶æ€å›¾æ ‡
    ICONS = {
        'start': 'ğŸš€', 'data': 'ğŸ“Š', 'train': 'ğŸ¯', 'model': 'ğŸ§ ',
        'done': 'âœ…', 'best': 'ğŸ†', 'file': 'ğŸ“', 'chart': 'ğŸ“ˆ',
        'time': 'â±ï¸', 'warn': 'âš ï¸', 'info': 'â„¹ï¸'
    }
    
    def __init__(self):
        self.start_time = time.time()
        self._spinner_active = False
        self._spinner_thread: Optional[threading.Thread] = None
    
    def header(self, title: str, char: str = "=", width: int = 65) -> None:
        """æ‰“å°æ ‡é¢˜å¤´"""
        print(f"\n{char * width}")
        print(f"  {title}")
        print(f"{char * width}")
    
    def section(self, title: str, icon: str = 'info') -> None:
        """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
        ico = self.ICONS.get(icon, icon)
        print(f"\n{ico} {title}")
        print("-" * 50)
    
    def step(self, msg: str, indent: int = 2) -> None:
        """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
        print(f"{' ' * indent}â†’ {msg}")
    
    def result(self, name: str, metrics: Dict[str, Any], indent: int = 4) -> None:
        """æ‰“å°å•ä¸ªæ¨¡å‹è®­ç»ƒç»“æœ"""
        acc = metrics.get('balanced_accuracy', 0)
        f1 = metrics.get('f1_macro', 0)
        t = metrics.get('train_time_seconds', 0)
        
        # æ€§èƒ½ç­‰çº§é¢œè‰²æ ‡è®°
        grade = "â˜…â˜…â˜…" if acc >= 0.85 else "â˜…â˜…â˜†" if acc >= 0.75 else "â˜…â˜†â˜†"
        print(f"{' ' * indent}âœ“ {name:20s} | Acc: {acc:.4f} | F1: {f1:.4f} | {t:5.1f}s | {grade}")
    
    def best_model(self, task: str, name: str, score: float) -> None:
        """æ‰“å°æœ€ä½³æ¨¡å‹ä¿¡æ¯"""
        print(f"\n  {self.ICONS['best']} æœ€ä½³{task}æ¨¡å‹: {name}")
        print(f"     ç»¼åˆå¾—åˆ†: {score:.4f}")
    
    def summary(self, risk_name: str, risk_acc: float, 
                dep_name: str, dep_acc: float, composite_mean: float, composite_std: float) -> None:
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        elapsed = time.time() - self.start_time
        print(f"\n{self.ICONS['chart']} æ¨¡å‹æ€§èƒ½æ¦‚è§ˆ:")
        print(f"   â”œâ”€ é£é™©æ¨¡å‹ ({risk_name}): {risk_acc:.2%}")
        print(f"   â””â”€ æŠ‘éƒæ¨¡å‹ ({dep_name}): {dep_acc:.2%}")
        print(f"\n{self.ICONS['chart']} ç»¼åˆè¯„åˆ†åˆ†å¸ƒ: Î¼={composite_mean:.3f}, Ïƒ={composite_std:.3f}")
        print(f"\n{self.ICONS['time']} æ€»è€—æ—¶: {elapsed:.1f}s")
    
    def file_output(self, files: List[str]) -> None:
        """æ‰“å°è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""
        print(f"\n{self.ICONS['file']} è¾“å‡ºæ–‡ä»¶:")
        for f in files:
            print(f"   â””â”€ {f}")
    
    def done(self) -> None:
        """æ‰“å°å®Œæˆä¿¡æ¯"""
        self.header(f"{self.ICONS['done']} è®­ç»ƒå®Œæˆ!", char="â•")


class TrainingSpinner:
    """å¼‚æ­¥è®­ç»ƒè¿›åº¦æŒ‡ç¤ºå™¨ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹æ— å¡é¡¿æ„Ÿ"""
    
    SPINNER_CHARS = ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â ']
    
    def __init__(self, message: str = "è®­ç»ƒä¸­"):
        self.message = message
        self._active = False
        self._thread: Optional[threading.Thread] = None
    
    def _spin(self) -> None:
        """åå°æ—‹è½¬åŠ¨ç”»"""
        idx = 0
        while self._active:
            char = self.SPINNER_CHARS[idx % len(self.SPINNER_CHARS)]
            sys.stdout.write(f"\r  {char} {self.message}...")
            sys.stdout.flush()
            time.sleep(0.1)
            idx += 1
        # æ¸…é™¤spinnerè¡Œ
        sys.stdout.write("\r" + " " * 50 + "\r")
        sys.stdout.flush()
    
    def start(self) -> None:
        """å¯åŠ¨spinner"""
        self._active = True
        self._thread = threading.Thread(target=self._spin, daemon=True)
        self._thread.start()
    
    def stop(self) -> None:
        """åœæ­¢spinner"""
        self._active = False
        if self._thread:
            self._thread.join(timeout=0.5)


# ========== åˆ—é‡å‘½åæ˜ å°„ ==========
COL_RENAME = {
    'Timestamp': 'timestamp',
    '1. What is your age?': 'age',
    '2. Gender': 'gender',
    '3. Relationship Status': 'relationship',
    '4. Occupation Status': 'occupation',
    '5. What type of organizations are you affiliated with?': 'affiliate_organization',
    '6. Do you use social media?': 'social_media_use',
    '7. What social media platforms do you commonly use?': 'platforms',
    '8. What is the average time you spend on social media every day?': 'avg_time_per_day',
    '9. How often do you find yourself using Social media without a specific purpose?': 'without_purpose',
    '10. How often do you get distracted by Social media when you are busy doing something?': 'distracted',
    "11. Do you feel restless if you haven't used Social media in a while?": 'restless',
    '12. On a scale of 1 to 5, how easily distracted are you?': 'distracted_ease',
    '13. On a scale of 1 to 5, how much are you bothered by worries?': 'worries',
    '14. Do you find it difficult to concentrate on things?': 'concentration',
    '15. On a scale of 1-5, how often do you compare yourself to other successful people through the use of social media?': 'compare_to_others',
    '16. Following the previous question, how do you feel about these comparisons, generally speaking?': 'compare_feelings',
    '17. How often do you look to seek validation from features of social media?': 'validation',
    '18. How often do you feel depressed or down?': 'depressed',
    '19. On a scale of 1 to 5, how frequently does your interest in daily activities fluctuate?': 'daily_activity_flux',
    '20. On a scale of 1 to 5, how often do you face issues regarding sleep?': 'sleeping_issues'
}

# ========== æ•°æ®åˆ—é…ç½® ==========
LIKERT_COLS = [
    'without_purpose', 'distracted', 'restless', 'distracted_ease', 'worries',
    'concentration', 'compare_to_others', 'compare_feelings', 'validation',
    'depressed', 'daily_activity_flux', 'sleeping_issues'
]

AVG_TIME_ORDER = [
    'Less than an Hour',
    'Between 1 and 2 hours',
    'Between 2 and 3 hours',
    'Between 3 and 4 hours',
    'Between 4 and 5 hours',
    'More than 5 hours'
]


# ========== å·¥å…·å‡½æ•° ==========
def compute_percentile(val: float, series: pd.Series) -> Optional[float]:
    """è®¡ç®—å€¼åœ¨åºåˆ—ä¸­çš„ç™¾åˆ†ä½æ•°"""
    if series is None or series.empty or val is None:
        return None
    q = np.nanpercentile(series, np.arange(0, 101))
    pct = np.interp(val, q, np.arange(0, 101))
    return round(float(np.clip(pct, 0, 100)), 1)


def load_and_clean() -> Tuple[pd.DataFrame, List[str]]:
    """
    åŠ è½½å¹¶æ¸…æ´—æ•°æ®é›†
    
    Returns:
        df: æ¸…æ´—åçš„DataFrame
        platforms: æ‰€æœ‰ç¤¾äº¤åª’ä½“å¹³å°åˆ—è¡¨
    """
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {DATA_PATH}")
    
    df = pd.read_csv(DATA_PATH)
    df = df.rename(columns=COL_RENAME)
    
    # å¡«å……ç¼ºå¤±çš„ç»„ç»‡ç±»å‹
    if df['affiliate_organization'].isnull().any():
        mode_val = df['affiliate_organization'].value_counts().index[0]
        df['affiliate_organization'] = df['affiliate_organization'].fillna(mode_val)
    
    # æ€§åˆ«æ ‡å‡†åŒ–
    df['gender'] = df['gender'].apply(lambda x: x if x in ['Male', 'Female'] else 'other')
    
    # å¹³å°ç‰¹å¾å·¥ç¨‹
    platform_dummies = df['platforms'].fillna('').apply(
        lambda x: [p.strip() for p in str(x).split(',') if p.strip()]
    )
    all_platforms = sorted(set(p for lst in platform_dummies for p in lst))
    
    for p in all_platforms:
        df[f'plat_{p}'] = platform_dummies.apply(lambda lst: 1 if p in lst else 0)
    df['platform_count'] = df[[f'plat_{p}' for p in all_platforms]].sum(axis=1)
    
    # æ•°å­—æˆç˜¾è¯„åˆ†
    q_cols = ['without_purpose', 'distracted', 'restless', 'distracted_ease']
    for c in q_cols:
        if c not in df.columns:
            df[c] = 0
    df['digital_addiction_score'] = df[q_cols].sum(axis=1)
    
    # æ—¶é—´ç‰¹å¾ç¼–ç 
    df['avg_time_per_day'] = pd.Categorical(df['avg_time_per_day'], categories=AVG_TIME_ORDER, ordered=True)
    df['avg_time_ord'] = df['avg_time_per_day'].cat.codes.replace(-1, pd.NA).astype('float')
    df['avg_time_ord'] = df['avg_time_ord'].fillna(df['avg_time_ord'].median())
    
    # é£é™©æ ‡ç­¾
    df['impact_sum'] = df[LIKERT_COLS].sum(axis=1)
    df['risk'] = np.where(df['impact_sum'] >= 37, 'higher', 'lower')
    
    # ç±»åˆ«ç¼–ç 
    df['relationship_enc'] = pd.factorize(df['relationship'].fillna('unknown'))[0]
    df['occupation_enc'] = pd.factorize(df['occupation'].fillna('unknown'))[0]
    
    # æ€§åˆ«ç‹¬çƒ­ç¼–ç 
    gender_dummies = pd.get_dummies(df['gender'].fillna('other'), prefix='gender')
    for c in gender_dummies.columns:
        df[c] = gender_dummies[c]
    
    return df, all_platforms


def get_model_candidates() -> Dict[str, Tuple[Any, Dict]]:
    """
    è·å–æ‰€æœ‰å€™é€‰æ¨¡å‹åŠå…¶è¶…å‚æ•°ç½‘æ ¼
    
    Returns:
        å€™é€‰æ¨¡å‹å­—å…¸: {æ¨¡å‹å: (ä¼°è®¡å™¨å®ä¾‹, å‚æ•°ç½‘æ ¼)}
    """
    candidates = {
        'LogisticRegression': (
            LogisticRegression(max_iter=500, random_state=42),
            {'model__C': [0.1, 1.0, 10.0], 'model__solver': ['lbfgs', 'liblinear']}
        ),
        'RandomForest': (
            RandomForestClassifier(random_state=42, n_jobs=-1),
            {'model__n_estimators': [100, 200], 'model__max_depth': [6, 12, None], 'model__min_samples_leaf': [1, 2]}
        ),
        'GradientBoosting': (
            GradientBoostingClassifier(random_state=42),
            {'model__n_estimators': [100, 150], 'model__max_depth': [3, 5], 'model__learning_rate': [0.05, 0.1]}
        ),
        'SVM': (
            SVC(probability=True, random_state=42),
            {'model__C': [0.1, 1.0, 10.0], 'model__kernel': ['rbf', 'linear']}
        )
    }
    
    if HAS_XGB:
        candidates['XGBoost'] = (
            XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, verbosity=0, n_jobs=-1),
            {'model__n_estimators': [100, 200], 'model__max_depth': [3, 6], 'model__learning_rate': [0.05, 0.1]}
        )
    
    if HAS_LGBM:
        candidates['LightGBM'] = (
            LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1),
            {'model__n_estimators': [100, 200], 'model__max_depth': [3, 6], 'model__learning_rate': [0.05, 0.1]}
        )
    
    return candidates


def train_single_model(
    X_train: pd.DataFrame, X_test: pd.DataFrame, 
    y_train: pd.Series, y_test: pd.Series,
    preprocessor: ColumnTransformer, 
    model_name: str, estimator: Any, param_grid: Dict,
    is_binary: bool = True
) -> Tuple[Dict[str, Any], Any]:
    """
    è®­ç»ƒå•ä¸ªæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½
    
    Args:
        X_train, X_test: è®­ç»ƒ/æµ‹è¯•ç‰¹å¾
        y_train, y_test: è®­ç»ƒ/æµ‹è¯•æ ‡ç­¾  
        preprocessor: æ•°æ®é¢„å¤„ç†å™¨
        model_name: æ¨¡å‹åç§°
        estimator: sklearnä¼°è®¡å™¨
        param_grid: è¶…å‚æ•°ç½‘æ ¼
        is_binary: æ˜¯å¦ä¸ºäºŒåˆ†ç±»ä»»åŠ¡
        
    Returns:
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        best_model: è®­ç»ƒåçš„æœ€ä½³æ¨¡å‹
    """
    start_time = time.time()
    
    # æ„å»ºPipeline
    pipe = Pipeline([
        ('prep', preprocessor), 
        ('model', estimator)
    ])
    
    # ç½‘æ ¼æœç´¢ (ä½¿ç”¨å¹¶è¡ŒåŠ é€Ÿ)
    search = GridSearchCV(
        pipe, param_grid, 
        cv=5, 
        scoring='balanced_accuracy', 
        n_jobs=-1,  # å¹¶è¡ŒåŠ é€Ÿ
        error_score='raise'
    )
    search.fit(X_train, y_train)
    
    best_model = search.best_estimator_
    train_time = time.time() - start_time
    
    # é¢„æµ‹ä¸è¯„ä¼°
    y_pred = best_model.predict(X_test)
    y_proba = None
    if hasattr(best_model, "predict_proba"):
        try:
            y_proba = best_model.predict_proba(X_test)
        except Exception:
            pass

    # æ„å»ºæŒ‡æ ‡å­—å…¸
    metrics = {
        'model_name': model_name,
        'best_params': search.best_params_,
        'train_time_seconds': round(train_time, 2),
        'cv_score': round(search.best_score_, 4),
        'balanced_accuracy': round(balanced_accuracy_score(y_test, y_pred), 4),
        'f1_macro': round(f1_score(y_test, y_pred, average='macro'), 4),
        'precision_macro': round(precision_score(y_test, y_pred, average='macro', zero_division=0), 4),
        'recall_macro': round(recall_score(y_test, y_pred, average='macro', zero_division=0), 4),
    }

    # äºŒåˆ†ç±»é¢å¤–è®¡ç®—ROC-AUC
    if is_binary and y_proba is not None:
        try:
            metrics['roc_auc'] = round(roc_auc_score(y_test, y_proba[:, 1]), 4)
        except Exception:
            pass
    
    metrics['classification_report'] = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
    
    return metrics, best_model


def train_all_models(
    X_train: pd.DataFrame, X_test: pd.DataFrame,
    y_train: pd.Series, y_test: pd.Series,
    preprocessor: ColumnTransformer,
    model_candidates: Dict[str, Tuple[Any, Dict]],
    is_binary: bool,
    printer: ProgressPrinter
) -> Tuple[List[Dict], Dict[str, Any]]:
    """
    è®­ç»ƒæ‰€æœ‰å€™é€‰æ¨¡å‹ (å¸¦è¿›åº¦æ˜¾ç¤º)
    
    Args:
        X_train, X_test, y_train, y_test: è®­ç»ƒæµ‹è¯•æ•°æ®
        preprocessor: é¢„å¤„ç†å™¨
        model_candidates: å€™é€‰æ¨¡å‹å­—å…¸
        is_binary: æ˜¯å¦äºŒåˆ†ç±»
        printer: è¿›åº¦æ‰“å°å™¨
        
    Returns:
        results: æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°ç»“æœåˆ—è¡¨
        models: è®­ç»ƒå¥½çš„æ¨¡å‹å­—å…¸
    """
    results = []
    models = {}
    total = len(model_candidates)
    
    for idx, (name, (estimator, params)) in enumerate(model_candidates.items(), 1):
        printer.step(f"[{idx}/{total}] è®­ç»ƒ {name}...")
        
        # ä½¿ç”¨spinneræ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        spinner = TrainingSpinner(f"è®­ç»ƒ {name}")
        spinner.start()
        
        try:
            metrics, model = train_single_model(
                X_train, X_test, y_train, y_test,
                preprocessor, name, estimator, params, is_binary
            )
            results.append(metrics)
            models[name] = model
        finally:
            spinner.stop()
        
        # æ‰“å°ç»“æœ
        printer.result(name, metrics)
    
    return results, models


def select_best_model(results: List[Dict]) -> Tuple[str, Dict]:
    """
    æ ¹æ®ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä½³æ¨¡å‹
    
    è¯„åˆ†å…¬å¼: composite = balanced_accuracy * 0.4 + f1_macro * 0.3 + cv_score * 0.3
    
    Args:
        results: æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœ
        
    Returns:
        best_name: æœ€ä½³æ¨¡å‹åç§°
        best_metrics: æœ€ä½³æ¨¡å‹æŒ‡æ ‡
    """
    for r in results:
        r['composite_score'] = (
            r.get('balanced_accuracy', 0) * 0.4 +
            r.get('f1_macro', 0) * 0.3 +
            r.get('cv_score', 0) * 0.3
        )
    
    best = max(results, key=lambda x: x.get('composite_score', 0))
    return best.get('model_name', 'unknown'), best


def save_training_report(
    model_info: Dict, training_report: Dict, 
    output_dir: str
) -> List[str]:
    """
    ä¿å­˜æ¨¡å‹ä¿¡æ¯å’Œè®­ç»ƒæŠ¥å‘Š
    
    Args:
        model_info: æ¨¡å‹å…ƒä¿¡æ¯
        training_report: è¯¦ç»†è®­ç»ƒæŠ¥å‘Š
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        saved_files: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    files = []
    
    info_path = os.path.join(output_dir, 'smmh_model_info.json')
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    files.append(info_path)
    
    report_path = os.path.join(output_dir, 'model_comparison_report.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(training_report, f, ensure_ascii=False, indent=2)
    files.append(report_path)
    
    return files


# ========== ä¸»å‡½æ•° ==========
def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    printer = ProgressPrinter()
    
    # ===== åˆå§‹åŒ– =====
    printer.header("MindScreen - å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒ", char="â•")
    print(f"  å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ£€æµ‹å¯ç”¨æ¨¡å‹
    available = ['LogisticRegression', 'RandomForest', 'GradientBoosting', 'SVM']
    if HAS_XGB:
        available.append('XGBoost')
    if HAS_LGBM:
        available.append('LightGBM')
    print(f"  å¯ç”¨æ¨¡å‹: {', '.join(available)}")
    
    # ===== æ•°æ®åŠ è½½ =====
    printer.section("åŠ è½½ä¸é¢„å¤„ç†æ•°æ®", icon='data')
    df, platforms = load_and_clean()
    os.makedirs(MODEL_DIR, exist_ok=True)
    printer.step(f"æ•°æ®é›†: {len(df)} è¡Œ, {len(platforms)} ä¸ªå¹³å°")
    
    # åŸºçº¿ç»Ÿè®¡
    baseline_map = {}
    for col in LIKERT_COLS:
        series = df[col]
        mean_val = float(series.mean()) if not series.empty else None
        baseline_map[col] = {
            'baseline_value': mean_val,
            'baseline_percentile': compute_percentile(mean_val, series) if mean_val is not None else None
        }
    
    # ç‰¹å¾å®šä¹‰
    plat_cols = [f'plat_{p}' for p in platforms]
    gender_cols = [c for c in df.columns if c.startswith('gender_')]
    feature_cols = ['age', 'relationship_enc', 'occupation_enc', 'avg_time_ord',
                    'platform_count', 'digital_addiction_score'] + plat_cols + gender_cols
    
    num_cols = ['age', 'platform_count', 'digital_addiction_score', 'avg_time_ord',
                'relationship_enc', 'occupation_enc']
    cat_cols = plat_cols + gender_cols
    
    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),
    ])
    
    printer.step(f"ç‰¹å¾æ•°: {len(feature_cols)} (æ•°å€¼: {len(num_cols)}, ç±»åˆ«: {len(cat_cols)})")
    
    model_candidates = get_model_candidates()
    
    # ===== é£é™©æ¨¡å‹è®­ç»ƒ =====
    printer.section("è®­ç»ƒé£é™©é¢„æµ‹æ¨¡å‹ (äºŒåˆ†ç±»)", icon='train')
    
    X_risk = df[feature_cols]
    y_risk_raw = df['risk']
    risk_mapping = {'higher': 1, 'lower': 0}
    y_risk = y_risk_raw.map(risk_mapping)
    
    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
        X_risk, y_risk, test_size=0.2, random_state=42, stratify=y_risk
    )
    
    risk_results, risk_models = train_all_models(
        X_train_r, X_test_r, y_train_r, y_test_r,
        preprocessor, model_candidates, is_binary=True, printer=printer
    )
    
    best_risk_name, best_risk_metrics = select_best_model(risk_results)
    printer.best_model("é£é™©", best_risk_name, best_risk_metrics.get('composite_score', 0))
    
    risk_model = risk_models[best_risk_name]
    risk_model_path = os.path.join(MODEL_DIR, 'smmh_risk_pipeline_v2.pkl')
    joblib.dump(risk_model, risk_model_path)
    
    # ===== æŠ‘éƒæ¨¡å‹è®­ç»ƒ =====
    printer.section("è®­ç»ƒæŠ‘éƒç­‰çº§æ¨¡å‹ (å¤šåˆ†ç±» 1-5)", icon='model')
    
    X_dep = df[feature_cols]
    y_dep_raw = df['depressed']
    dep_mapping = {lab: i for i, lab in enumerate(sorted(y_dep_raw.unique()))}
    y_dep = y_dep_raw.map(dep_mapping)
    
    X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(
        X_dep, y_dep, test_size=0.2, random_state=42, stratify=y_dep
    )
    
    dep_results, dep_models = train_all_models(
        X_train_d, X_test_d, y_train_d, y_test_d,
        preprocessor, model_candidates, is_binary=False, printer=printer
    )
    
    best_dep_name, best_dep_metrics = select_best_model(dep_results)
    printer.best_model("æŠ‘éƒ", best_dep_name, best_dep_metrics.get('composite_score', 0))
    
    dep_model = dep_models[best_dep_name]
    dep_model_path = os.path.join(MODEL_DIR, 'smmh_depressed_pipeline.pkl')
    joblib.dump(dep_model, dep_model_path)
    
    # ===== ç»¼åˆè¯„åˆ†è®¡ç®— =====
    printer.section("è®¡ç®—ç»¼åˆè¯„åˆ†åˆ†å¸ƒ", icon='chart')
    X_all = df[feature_cols]
    
    risk_proba = risk_model.predict_proba(X_all)
    higher_idx = risk_mapping['higher']
    risk_prob_higher = risk_proba[:, higher_idx]
    
    dep_pred = dep_model.predict(X_all)
    dep_numeric = dep_pred.astype(float) + 1
    
    composite_scores = 0.5 * risk_prob_higher + 0.5 * (dep_numeric / 5.0)
    composite_percentiles = np.percentile(composite_scores, np.arange(0, 101))
    
    printer.step(f"è¯„åˆ†èŒƒå›´: [{np.min(composite_scores):.3f}, {np.max(composite_scores):.3f}]")
    printer.step(f"è¯„åˆ†åˆ†å¸ƒ: Î¼={np.mean(composite_scores):.3f}, Ïƒ={np.std(composite_scores):.3f}")
    
    # ===== ä¿å­˜æŠ¥å‘Š =====
    model_info = {
        'dataset': 'smmh.csv',
        'n_rows': len(df),
        'features': feature_cols,
        'platforms': platforms,
        'likert_baseline': baseline_map,
        'risk': {
            'model': best_risk_name,
            'params': best_risk_metrics.get('best_params'),
            'balanced_accuracy': best_risk_metrics.get('balanced_accuracy'),
            'f1_macro': best_risk_metrics.get('f1_macro'),
            'roc_auc': best_risk_metrics.get('roc_auc'),
            'report': best_risk_metrics.get('classification_report')
        },
        'depressed': {
            'model': best_dep_name,
            'params': best_dep_metrics.get('best_params'),
            'balanced_accuracy': best_dep_metrics.get('balanced_accuracy'),
            'f1_macro': best_dep_metrics.get('f1_macro'),
            'report': best_dep_metrics.get('classification_report')
        },
        'composite_score_distribution': {
            'percentiles': composite_percentiles.tolist(),
            'mean': float(np.mean(composite_scores)),
            'std': float(np.std(composite_scores)),
            'min': float(np.min(composite_scores)),
            'max': float(np.max(composite_scores)),
            'formula': 'composite = 0.5 * P(risk=higher) + 0.5 * (depressed_level / 5)'
        }
    }
    
    training_report = {
        'training_date': datetime.now().isoformat(),
        'dataset_info': {
            'path': DATA_PATH,
            'rows': len(df),
            'features': len(feature_cols)
        },
        'risk_model_comparison': risk_results,
        'depressed_model_comparison': dep_results,
        'best_models': {
            'risk': {
                'name': best_risk_name,
                'reason': f"ç»¼åˆè¯„åˆ†æœ€é«˜ ({best_risk_metrics.get('composite_score', 0):.4f})"
            },
            'depressed': {
                'name': best_dep_name,
                'reason': f"ç»¼åˆè¯„åˆ†æœ€é«˜ ({best_dep_metrics.get('composite_score', 0):.4f})"
            }
        },
        'model_selection_criteria': {
            'formula': 'composite_score = balanced_accuracy * 0.4 + f1_macro * 0.3 + cv_score * 0.3',
            'reasoning': 'ç»¼åˆè€ƒè™‘æµ‹è¯•é›†å‡†ç¡®ç‡ã€F1åˆ†æ•°å’Œäº¤å‰éªŒè¯ç¨³å®šæ€§'
        }
    }
    
    report_files = save_training_report(model_info, training_report, MODEL_DIR)
    
    # ===== è¾“å‡ºæ€»ç»“ =====
    all_output_files = [risk_model_path, dep_model_path] + report_files
    printer.file_output(all_output_files)
    
    printer.summary(
        best_risk_name, best_risk_metrics.get('balanced_accuracy', 0),
        best_dep_name, best_dep_metrics.get('balanced_accuracy', 0),
        np.mean(composite_scores), np.std(composite_scores)
    )
    
    printer.done()


if __name__ == '__main__':
    main()
